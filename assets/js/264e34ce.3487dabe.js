"use strict";(self.webpackChunk_docsearch_website=self.webpackChunk_docsearch_website||[]).push([[6299],{5260:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"inside-the-engine","title":"Inside the engine","description":"This page explains in more detail how the crawler extracts content from your page, and how it ranks the results.","source":"@site/versioned_docs/version-legacy/inside-the-engine.md","sourceDirName":".","slug":"/inside-the-engine","permalink":"/docs/legacy/inside-the-engine","draft":false,"unlisted":false,"tags":[],"version":"legacy","frontMatter":{"title":"Inside the engine"},"sidebar":"docs","previous":{"title":"Supported Integrations","permalink":"/docs/legacy/integrations"},"next":{"title":"Config Files","permalink":"/docs/legacy/config-file"}}');var r=t(4848),s=t(8453);const a={title:"Inside the engine"},o=void 0,c={},l=[{value:"Crawling",id:"crawling",level:2},{value:"Extracting content",id:"extracting-content",level:2},{value:"Ranking records",id:"ranking-records",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"This page explains in more detail how the crawler extracts content from your page, and how it ranks the results."}),"\n",(0,r.jsx)(n.h2,{id:"crawling",children:"Crawling"}),"\n",(0,r.jsxs)(n.p,{children:["Each crawl begins its journey at ",(0,r.jsx)(n.code,{children:"start_urls"})," value specified in your config. It will read those pages, recursively extract and follow every link in those pages until it has browsed every compliant page."]}),"\n",(0,r.jsxs)(n.p,{children:["If you have explicitly defined a ",(0,r.jsx)(n.code,{children:"sitemap.xml"}),", our crawler will scrape every provided and compliant page. We recommend using ",(0,r.jsx)(n.a,{href:"https://www.sitemaps.org/",children:"a sitemap"})," since it explicitly exposes URLs to crawl and avoid missing pages that aren't linked from another page."]}),"\n",(0,r.jsx)(n.h2,{id:"extracting-content",children:"Extracting content"}),"\n",(0,r.jsx)(n.p,{children:"Building records using the scraper is pretty intuitive. Based on your settings, we extract the payload of your web page and index it, preserving your data structure. It achieves this in a simple way:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["We ",(0,r.jsx)(n.strong,{children:"read top down"})," your web page following your HTML flow and pick out your matching elements according to their ",(0,r.jsx)(n.strong,{children:"levels"})," based on the ",(0,r.jsx)(n.code,{children:"selectors_level"})," defined."]}),"\n",(0,r.jsxs)(n.li,{children:["We create a record for each paragraph along with its hierarchical path. This construction is based on their ",(0,r.jsx)(n.strong,{children:"time of appearance"})," along the flow."]}),"\n",(0,r.jsxs)(n.li,{children:["We ",(0,r.jsx)(n.strong,{children:"index"})," these records with the appropriate global settings (e.g. metadata, tags, etc.)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," The above process performs sanity tests as it scrapes to detect errors. If there are any serious warnings, it aborts and hence does not overwrite your current index. These checks ensure that your dedicated index isn't flushed."]})}),"\n",(0,r.jsxs)(n.p,{children:["You can ",(0,r.jsx)(n.a,{href:"/docs/legacy/how-do-we-build-an-index",children:"find more explanations in this dedicated section."})]}),"\n",(0,r.jsx)(n.h2,{id:"ranking-records",children:"Ranking records"}),"\n",(0,r.jsxs)(n.p,{children:["Algolia always returns the most relevant results first, using a ",(0,r.jsx)(n.a,{href:"https://www.algolia.com/doc/guides/ranking/ranking-formula/#tie-breaking-approach",children:"tie-breaking approach"}),". DocSearch will first search for exact matches in your keywords, and then fallback to partial matches. It sorts those results, once again, on the page hierarchy, as extracted from the ",(0,r.jsx)(n.code,{children:"selectors"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The default strategy is to promote records having matching words in the highest level first. Thus if two results have the same matching words, the one having them in the highest level (",(0,r.jsx)(n.code,{children:"lvl0"}),") will be ranked higher. We also use the position of the matching words. The sooner they appear within the HTML flow, the higher the record will be ranked."]}),"\n",(0,r.jsx)(n.p,{children:"We base relevancy on several factors and customize it according to the Algolia tie-breaking method."}),"\n",(0,r.jsxs)(n.p,{children:["You can boost pages depending on their URLs. You should use the ",(0,r.jsx)(n.code,{children:"start_urls"})," and its ",(0,r.jsx)(n.code,{children:"page_rank"})," attributes. Its value is a numeric value (defaults to 0). The higher the value is, the higher results from the matching pages are ranked. For example, all pages with a ",(0,r.jsx)(n.code,{children:"page_rank"})," of 5 will be returned before pages with a ",(0,r.jsx)(n.code,{children:"page_rank"})," of 1."]}),"\n",(0,r.jsxs)(n.p,{children:["You could even change the relevancy strategy by ",(0,r.jsxs)(n.a,{href:"https://www.algolia.com/doc/guides/ranking/custom-ranking/",children:["overwriting the default ",(0,r.jsx)(n.code,{children:"customRanking"})]})," used by the index by using the ",(0,r.jsx)(n.code,{children:"custom_settings"})," option of your config."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);